{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import pyspark\\nfrom pyspark.sql.types import *\\nfrom pyspark.ml.tuning import TrainValidationSplit\\nfrom pyspark.ml.recommendation import ALS\\nfrom pyspark.ml.evaluation import RegressionEvaluator'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/Users/shiahlints/galvanize/FIXGITPROBLEM/booking-agent-ai')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import find_nearest_venues as fnv\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "'''import pyspark\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.tuning import TrainValidationSplit\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_events = pd.read_pickle('../data/train_events_df.p')\n",
    "test_events = pd.read_pickle('../data/test_events_raw_df.p')\n",
    "train_venues = pd.read_pickle('../data/train_venues_df.p')\n",
    "artists = pd.read_pickle('../data/master_artists_df.p')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We start by sampling form the test events\n",
    "we will use the venue id to get the nearest 100 venues to where this \n",
    "event took place. We will then check if the model return this venues \n",
    "as one of the top ten recomendations. The model is trained on the train events.\n",
    "The data has been split in such a way that there is no leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_samples = 50\n",
    "samples = test_events.sample(number_of_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calibrate the vunue finder\n",
    "The venue finder is calibrated with all the venues\n",
    "in the training set. This was it will return venues only in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vf = fnv.Venue_Finder(train_venues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make a dataset to train the model on \n",
    "and filter out venues without bios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = train_events[['artist_id', 'venue_bio', 'venue_id']]\n",
    "data = data[~data.venue_bio.isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a count vectorizer and a MultinomialNb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words = 'english', max_features = 1000)\n",
    "counts = vectorizer.fit_transform(data['venue_bio'].values)\n",
    "classifier = MultinomialNB()\n",
    "targets = data['venue_id'].values\n",
    "classifier.fit(counts, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vectorizer',  CountVectorizer(stop_words = 'english', max_features = 1000)),\n",
    "    ('classifier',  MultinomialNB()) ])\n",
    "\n",
    "pipeline.fit(data['venue_bio'].values, data['venue_id'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ratings1 = pipeline.predict_proba(['this is a test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_samples = 50\n",
    "samples = test_events.sample(number_of_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35294117647058826\n"
     ]
    }
   ],
   "source": [
    "\n",
    "new_venue = 0 \n",
    "model_sucesses = 0\n",
    "model_failures = 0\n",
    "number_venues_to_search = 100\n",
    "top_n_venues = 10\n",
    "no_bio = 0\n",
    "for i, event in samples.iterrows():\n",
    "    artist_id = event.artist_id\n",
    "    venue_id = event.venue_id\n",
    "    artist_bio = event.artist_bio\n",
    "    if venue_id not in train_venues.venue_id.values:\n",
    "        new_venue += 1\n",
    "        continue\n",
    "    \n",
    "    if artist_bio == None:\n",
    "        no_bio += 1\n",
    "        continue\n",
    "    else:\n",
    "        # First get a array of the classes. Predict probab only returns the probibility of \n",
    "        # document \"bio\" belonging to a class \"venue\". We are only interested in local \n",
    "        #venues so we need this array to filter the probibilities:\n",
    "        all_venues = classifier.classes_\n",
    "        \n",
    "        # get_nearest_venues_by_venue returns a data_frame contianing all the venue info\n",
    "        # we are only interested in the venue_id values\n",
    "        nearest_venues = (vf.get_nearest_venues_by_venue(venue_id, number_venues_to_search).\n",
    "                          venue_id.values)\n",
    "        \n",
    "        # these next to lines return a array of probabilities that relate to the \n",
    "        # classes in all_venues\n",
    "        bio_vectorized = vectorizer.transform([artist_bio])\n",
    "        ratings = classifier.predict_proba(bio_vectorized).reshape(-1,)\n",
    "        \n",
    "        #now we make a filter to get only the venues close by \n",
    "        venue_filter = np.isin(all_venues, nearest_venues)\n",
    "        \n",
    "        #and apply it on the venues and porbs\n",
    "        local_venues = all_venues[venue_filter]\n",
    "        local_probs = ratings[venue_filter]\n",
    "        \n",
    "        #Now make a sorter with the provs to sort the venues from most to least likely\n",
    "        sorter = local_probs.argsort()\n",
    "        top_local_venues_sorted = local_venues[sorter[:number_venues_to_search-10:-1]]\n",
    "    \n",
    "    \n",
    "        if venue_id in top_local_venues_sorted:\n",
    "            model_sucesses += 1\n",
    "        else:\n",
    "            model_failures += 1\n",
    "\n",
    "print(model_sucesses/(model_failures+model_sucesses))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thats better then .1 lest wright that all into a function that so we can test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_ten_rate(samples, classifier, vectorizer):\n",
    "    new_venue = 0 \n",
    "    model_sucesses = 0\n",
    "    model_failures = 0\n",
    "    number_venues_to_search = 100\n",
    "    top_n_venues = 10\n",
    "    no_bio = 0\n",
    "    for i, event in samples.iterrows():\n",
    "        artist_id = event.artist_id\n",
    "        venue_id = event.venue_id\n",
    "        artist_bio = event.artist_bio\n",
    "        if venue_id not in train_venues.venue_id.values:\n",
    "            new_venue += 1\n",
    "            continue\n",
    "\n",
    "        if artist_bio == None:\n",
    "            no_bio += 1\n",
    "            continue\n",
    "        else:\n",
    "            # First get a array of the classes. Predict probab only returns the probibility of \n",
    "            # document \"bio\" belonging to a class \"venue\". We are only interested in local \n",
    "            #venues so we need this array to filter the probibilities:\n",
    "            all_venues = classifier.classes_\n",
    "\n",
    "            # get_nearest_venues_by_venue returns a data_frame contianing all the venue info\n",
    "            # we are only interested in the venue_id values\n",
    "            nearest_venues = (vf.get_nearest_venues_by_venue(venue_id, number_venues_to_search).\n",
    "                              venue_id.values)\n",
    "\n",
    "            # these next to lines return a array of probabilities that relate to the \n",
    "            # classes in all_venues\n",
    "            bio_vectorized = vectorizer.transform([artist_bio])\n",
    "            ratings = classifier.predict_proba(bio_vectorized).reshape(-1,)\n",
    "\n",
    "            #now we make a filter to get only the venues close by \n",
    "            venue_filter = np.isin(all_venues, nearest_venues)\n",
    "\n",
    "            #and apply it on the venues and porbs\n",
    "            local_venues = all_venues[venue_filter]\n",
    "            local_probs = ratings[venue_filter]\n",
    "\n",
    "            #Now make a sorter with the provs to sort the venues from most to least likely\n",
    "            sorter = local_probs.argsort()\n",
    "            top_local_venues_sorted = local_venues[sorter[:number_venues_to_search-10:-1]]\n",
    "\n",
    "\n",
    "            if venue_id in top_local_venues_sorted:\n",
    "                model_sucesses += 1\n",
    "            else:\n",
    "                model_failures += 1\n",
    "\n",
    "    return(model_sucesses/(model_failures+model_sucesses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.20833333333333334,\n",
       " 0.2916666666666667,\n",
       " 0.21739130434782608,\n",
       " 0.18518518518518517,\n",
       " 0.35,\n",
       " 0.15789473684210525,\n",
       " 0.2631578947368421,\n",
       " 0.2857142857142857,\n",
       " 0.19047619047619047,\n",
       " 0.36363636363636365]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = []\n",
    "for i in range(10):\n",
    "    samples = test_events.sample(50)\n",
    "    results.append(top_ten_rate(samples, classifier, vectorizer))\n",
    "results   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thats a large deviation. \n",
    "lets take the mean of that and see what happens when we change the rand of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(features):\n",
    "    vectorizer = CountVectorizer(stop_words = 'english', max_features = features)\n",
    "    counts = vectorizer.fit_transform(data['venue_bio'].values)\n",
    "    classifier = MultinomialNB()\n",
    "    targets = data['venue_id'].values\n",
    "    classifier.fit(counts, targets)\n",
    "    return vectorizer, classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(vectorizer, classifier):\n",
    "    results = []\n",
    "    for i in range(10):\n",
    "        samples = test_events.sample(50)\n",
    "        results.append(top_ten_rate(samples, classifier, vectorizer))\n",
    "    print(results)\n",
    "    return np.mean(results) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1, 0.2608695652173913, 0.2631578947368421, 0.2857142857142857, 0.5238095238095238, 0.13043478260869565, 0.3157894736842105, 0.25, 0.25, 0.2608695652173913]\n",
      "[0.18518518518518517, 0.17391304347826086, 0.27586206896551724, 0.34615384615384615, 0.27586206896551724, 0.2, 0.2777777777777778, 0.2, 0.32, 0.21052631578947367]\n",
      "[0.3157894736842105, 0.2, 0.2916666666666667, 0.09523809523809523, 0.25, 0.2962962962962963, 0.125, 0.4166666666666667, 0.3076923076923077, 0.3333333333333333]\n",
      "[0.20833333333333334, 0.10526315789473684, 0.125, 0.2777777777777778, 0.26666666666666666, 0.20833333333333334, 0.17391304347826086, 0.14285714285714285, 0.3333333333333333, 0.2]\n",
      "[0.28, 0.3333333333333333, 0.2, 0.25, 0.16, 0.2, 0.20689655172413793, 0.19047619047619047, 0.2916666666666667, 0.21052631578947367]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.26406450909883405,\n",
       " 0.24652803063155782,\n",
       " 0.26316828395775765,\n",
       " 0.20414777886745847,\n",
       " 0.23228990579898018]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_list = [200, 400, 600, 800, 1000]\n",
    "results = []\n",
    "for features in features_list:\n",
    "    v, c = build_model(features)\n",
    "    results.append(test_model(v,c))\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a decent result but other aproaches are better so lets drop this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
